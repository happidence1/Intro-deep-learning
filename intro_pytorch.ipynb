{"cells":[{"cell_type":"markdown","metadata":{"id":"kWqGJ9inLeUN"},"source":["# Introduction to PyTorch\n","Texas A&M University Higher Performance Research Computing\n","\n","Reference: [Deep Learning with PyTorch: A 60 Minute Blitz ->Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTd7ONkWLeUO"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch"]},{"cell_type":"code","source":["print(torch.__version__)"],"metadata":{"id":"MdCygBUb-Ca8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79WTkPEwLeUP"},"source":["## 1. PyTorch as a fancy version of NumPy"]},{"cell_type":"markdown","metadata":{"id":"qtMbhr64LeUQ"},"source":["* PyTorch supports Tensor computation (like NumPy, a fundamental package for scientific computing with Python) with strong GPU acceleration. You can find corresponding numpy functions in PyTorch.\n","* It supports deep neural networks built on a tape-based autograd system"]},{"cell_type":"markdown","metadata":{"id":"pKmMJo11LeUQ"},"source":["### NumPy examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j85uG6OlLeUQ"},"outputs":[],"source":["x = np.array([[1,2,3],[3,4,5]], dtype=np.float64)\n","x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujuRxAzkLeUR"},"outputs":[],"source":["x = np.zeros([5,5])\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHEoJaXiLeUR"},"outputs":[],"source":["x = np.random.rand(5,5)\n","x"]},{"cell_type":"markdown","metadata":{"id":"ldgrp-2CLeUS"},"source":["### PyTorch examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQ2ENVIcLeUS"},"outputs":[],"source":["# create a 5X5 Torch tensor \n","x = torch.zeros(5,5)\n","x"]},{"cell_type":"code","source":["# create a 5X5 Torch tensor \n","x = torch.zeros([5,5])\n","x"],"metadata":{"id":"xj7qHgenEosN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdSaowR0LeUT"},"source":["### Check CUDA support"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZZbELH3LeUT"},"outputs":[],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufdScM9LLeUT"},"outputs":[],"source":["torch.cuda.device_count()"]},{"cell_type":"code","source":["torch.cuda.get_device_name()"],"metadata":{"id":"9EPT0iT9SEzj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppR1EHQMLeUU"},"outputs":[],"source":["dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgtDrHoALeUU"},"outputs":[],"source":["x = torch.rand((2,2), device=dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQsgEPVCLeUU"},"outputs":[],"source":["x.device"]},{"cell_type":"code","source":["y = torch.rand(2,2).to(dev)"],"metadata":{"id":"Q6GN7gXH-vZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwrCYLS0LeUV"},"outputs":[],"source":["y = torch.rand(2,2).to(x.device)\n","y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cafEz1nALeUV"},"outputs":[],"source":["y.device"]},{"cell_type":"markdown","metadata":{"id":"nqoVvTKkLeUV"},"source":["## 2. Autograd\n","\n","The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASRNllIELeUV"},"outputs":[],"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqDn_W6ALeUW"},"outputs":[],"source":["y = x + 2\n","print(y)\n","print(y.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QI8OpJhLeUW"},"outputs":[],"source":["print(y.grad_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIusHaXLLeUW"},"outputs":[],"source":["z = y * y * 3\n","out = z.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hV9lWH-NLeUW"},"outputs":[],"source":["out"]},{"cell_type":"code","source":["x.grad"],"metadata":{"id":"vDmfrdTxQHNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyNqSqZiLeUX"},"outputs":[],"source":["a = torch.randn(2, 2)\n","b = torch.randn(2, 2)\n","print(a)\n","print(b)"]},{"cell_type":"code","source":["# auto_grad is not activated by default\n","print(a.requires_grad)\n","print(b.requires_grad)"],"metadata":{"id":"wtux0giM3o7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.requires_grad_(True)\n","b.requires_grad_(True)\n","print(a.requires_grad)\n","print(b.requires_grad)"],"metadata":{"id":"W8ORbM_q3qTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c = (a**2 + b).sum()\n","print(c.grad_fn)"],"metadata":{"id":"dHvi8k0H3tsl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mo_AIU73LeUX"},"outputs":[],"source":["c.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkrmbEn5LeUX"},"outputs":[],"source":["print(a)"]},{"cell_type":"code","source":["a.grad"],"metadata":{"id":"lU0rng7zygIM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iNp1hWy1LeUX"},"source":["## 3. Neural Networks\n","\n","Neural networks can be constructed using the torch.nn package."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SnXyEjRLeUX"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","\n","net = Net()\n","print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bV7bp61DLeUY"},"outputs":[],"source":["input = torch.randn(1, 1, 32, 32)\n","out = net(input)\n","print(out)\n","out.shape"]},{"cell_type":"markdown","metadata":{"id":"rAeBoCU2LeUY"},"source":["### 3.1 Loss Function\n","There are several different loss functions under the nn package . A simple loss is: nn.MSELoss which computes the mean-squared error between the input and the target."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NReQhjVALeUY"},"outputs":[],"source":["output = net(input)\n","target = torch.randn(10)  # a dummy target, for example\n","print(target.shape)\n","target = target.view(1, -1)  # make it the same shape as output\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"]},{"cell_type":"markdown","source":["If you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations. For illustration, let us follow a few steps backward."],"metadata":{"id":"dJVAiMOb_ehF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pih3WP1iLeUY"},"outputs":[],"source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"]},{"cell_type":"markdown","metadata":{"id":"Km76dYkeLeUY"},"source":["### 3.2 Backprop\n","\n","To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ivg63a9LeUZ"},"outputs":[],"source":["net.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)"]},{"cell_type":"markdown","metadata":{"id":"N51MtLdALeUZ"},"source":["### 3.3 Update the weights\n","The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n","\n","weight = weight - learning_rate * gradient\n","\n","We can implement this using simple Python code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5fm67nsLeUZ"},"outputs":[],"source":["# a simple implementation\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"fHer9qN0LeUZ"},"source":["### 3.4 Optimizer\n","However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we can use a small package: torch.optim that implements all these methods. Using it is very simple:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDItSts4LeUZ"},"outputs":[],"source":["import torch.optim as optim\n","\n","# create your optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# in your training loop:\n","optimizer.zero_grad()   # zero the gradient buffers\n","output = net(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update"]},{"cell_type":"markdown","metadata":{"id":"y4Nk_vCxLeUa"},"source":["## 4. MNIST with Fully Connected Layers "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGKRcq6tLeUa"},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision import transforms, datasets\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OJDd5MmLeUa"},"outputs":[],"source":["# Use standard MNIST dataset\n","train = datasets.MNIST(\n","    root = './data/MNIST',\n","    train = True,\n","    download = True,\n","    transform = transforms.Compose([\n","        transforms.ToTensor()                                 \n","    ])\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp1oKqvhLeUa"},"outputs":[],"source":["test = datasets.MNIST(root = './data/MNIST', train=False, download=True, \n","                       transform=transforms.Compose([transforms.ToTensor()]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YaBy4mZLeUa"},"outputs":[],"source":["trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n","testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zr5Dg0eaLeUa"},"outputs":[],"source":["total = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QwYJFZpLeUa"},"outputs":[],"source":["counter_dict={0:0, 1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URul4dwbLeUa"},"outputs":[],"source":["total = 0\n","\n","for data in trainset:\n","    xs,ys = data\n","    for y in ys:\n","        counter_dict[int(y)] +=1\n","        total +=1\n","print (counter_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68LaQvq8LeUb"},"outputs":[],"source":["for i in counter_dict:\n","    print(f\"{i}: {counter_dict[i]/total*100}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-QYyoWmLeUb"},"outputs":[],"source":["import seaborn as sns\n","\n","sns.barplot(list(counter_dict.keys()), list(counter_dict.values()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QlB66hVLeUb"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odKyBkj-LeUb"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(28*28, 64)\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc3 = nn.Linear(64, 64)\n","        self.fc4 = nn.Linear(64, 10)\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        \n","        return F.log_softmax(x, dim=1)\n","        \n","net = Net()\n","print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIQk9VcpLeUc"},"outputs":[],"source":["optimizer = optim.Adam(net.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHElU-3dLeUc"},"outputs":[],"source":["epochs = 3\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AsqPOOTFLeUc"},"outputs":[],"source":["for epoch in tqdm(range(epochs)):\n","    for data in trainset:\n","        X,y = data\n","        net.zero_grad()\n","        output = net(X.view(-1, 28*28))\n","        loss = F.nll_loss(output, y)\n","        loss.backward()\n","        optimizer.step()\n","    print(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFBgbHg3LeUc"},"outputs":[],"source":["correct = 0 \n","total = 0\n","with torch.no_grad():\n","    for data in trainset:\n","        X, y =data\n","        output = net(X.view(-1, 784))\n","        for idx, i in enumerate(output):\n","            if torch.argmax(i) == y[idx]:\n","                correct +=1\n","            total +=1\n","print(\"Accuracy: \", round(correct/total,3))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkV5L6K7LeUc"},"outputs":[],"source":["plt.imshow(X[5].view(28,28))\n","print(torch.argmax(net(X[5].view(-1, 784))[0]))"]},{"cell_type":"code","source":[],"metadata":{"id":"dYifiPCAZ_1r"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[],"collapsed_sections":["rAeBoCU2LeUY","Km76dYkeLeUY","N51MtLdALeUZ","fHer9qN0LeUZ","y4Nk_vCxLeUa"]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}